{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/StebMart/FRALDEA-FaceRec/blob/main/FRALDEA_Project_(COSC_4394F24).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87RawKEh4Cyr"
      },
      "source": [
        "Use the [DroneFace dataset](https://huggingface.co/docs/transformers/) from here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6E-NCYQATZG",
        "outputId": "d0a13668-faa2-4d4b-8613-15057a57a1fb",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "%pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2hNbrOvAP9H"
      },
      "source": [
        "Install the necessary packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5ybjXAfxdcq",
        "outputId": "567f47a7-ea24-40a8-95ca-3e720faaaf93",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting retina-face\n",
            "  Downloading retina_face-0.0.17-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting tensorflow_addons\n",
            "  Downloading tensorflow_addons-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (1.0.11)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from retina-face) (1.26.4)\n",
            "Requirement already satisfied: gdown>=3.10.1 in /usr/local/lib/python3.10/dist-packages (from retina-face) (5.2.0)\n",
            "Requirement already satisfied: Pillow>=5.2.0 in /usr/local/lib/python3.10/dist-packages (from retina-face) (11.0.0)\n",
            "Requirement already satisfied: opencv-python>=3.4.4 in /usr/local/lib/python3.10/dist-packages (from retina-face) (4.10.0.84)\n",
            "Requirement already satisfied: tensorflow>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from retina-face) (2.17.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow_addons) (24.2)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow_addons)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from timm) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.20.1+cu121)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.26.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.5)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown>=3.10.1->retina-face) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown>=3.10.1->retina-face) (3.16.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown>=3.10.1->retina-face) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown>=3.10.1->retina-face) (4.66.6)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->retina-face) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->retina-face) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->retina-face) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->retina-face) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->retina-face) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->retina-face) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->retina-face) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->retina-face) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->retina-face) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->retina-face) (4.25.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->retina-face) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->retina-face) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->retina-face) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->retina-face) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->retina-face) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->retina-face) (1.68.0)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->retina-face) (2.17.1)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->retina-face) (3.5.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->retina-face) (0.37.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2024.10.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->timm) (1.3.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow>=1.9.0->retina-face) (0.45.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow>=1.9.0->retina-face) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow>=1.9.0->retina-face) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow>=1.9.0->retina-face) (0.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown>=3.10.1->retina-face) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown>=3.10.1->retina-face) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown>=3.10.1->retina-face) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown>=3.10.1->retina-face) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow>=1.9.0->retina-face) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow>=1.9.0->retina-face) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow>=1.9.0->retina-face) (3.1.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown>=3.10.1->retina-face) (2.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->timm) (3.0.2)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown>=3.10.1->retina-face) (1.7.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow>=1.9.0->retina-face) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow>=1.9.0->retina-face) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow>=1.9.0->retina-face) (0.1.2)\n",
            "Downloading retina_face-0.0.17-py3-none-any.whl (25 kB)\n",
            "Downloading tensorflow_addons-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (611 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.8/611.8 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, tensorflow_addons, retina-face\n",
            "  Attempting uninstall: typeguard\n",
            "    Found existing installation: typeguard 4.4.1\n",
            "    Uninstalling typeguard-4.4.1:\n",
            "      Successfully uninstalled typeguard-4.4.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "inflect 7.4.0 requires typeguard>=4.0.1, but you have typeguard 2.13.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed retina-face-0.0.17 tensorflow_addons-0.23.0 typeguard-2.13.3\n"
          ]
        }
      ],
      "source": [
        "!pip install retina-face tensorflow_addons timm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSNAEASBHAZ5"
      },
      "source": [
        "# Uploading dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "zqbMUWC7Lavl",
        "outputId": "120c874b-bd4a-4205-d4f5-52222a8bca81"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-cfa666fee151>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Mount Google Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Set the path to the zip file in Google Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Set the path to the zip file in Google Drive\n",
        "zip_path = '/content/drive/My Drive/Colab Notebooks/DnHFaces.zip'\n",
        "extract_to = '/content/DnHFaces'\n",
        "\n",
        "# Step 1: Unzip the file\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_to)\n",
        "\n",
        "print(f\"Dataset extracted to: {extract_to}\")\n",
        "\n",
        "# Verify the extracted files\n",
        "print(\"Contents of extracted folder:\")\n",
        "!ls \"/content/DnHFaces/open_data_set\"\n",
        "\n",
        "testing_methods = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpOAl0ZQHKv4"
      },
      "source": [
        "# Metadata Parsing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jHtxMRbQFffd"
      },
      "outputs": [],
      "source": [
        "def parse_filename(filename):\n",
        "    \"\"\"\n",
        "    Parse the filename to extract metadata such as subject ID, camera type, and image type.\n",
        "    \"\"\"\n",
        "    parts = filename.split(\"_\")\n",
        "    metadata = {\n",
        "        \"subject_id\": parts[0],  # Subject ID\n",
        "        \"camera_type\": parts[1] if len(parts) > 1 else \"na\",\n",
        "        \"height_id\": parts[2] if len(parts) > 2 else \"na\",\n",
        "        \"image_type\": parts[3] if len(parts) > 3 else \"na\",\n",
        "        \"distance_id\": parts[4].split(\".\")[0] if len(parts) > 4 else \"na\",\n",
        "    }\n",
        "    return metadata\n",
        "\n",
        "if testing_methods:\n",
        "    filename = \"a_gp_0_ef_01.jpg\"\n",
        "    metadata = parse_filename(filename)\n",
        "    print(metadata)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o282Ct3b_-mU"
      },
      "source": [
        "# Detect and Crop Faces Using RetinaFace\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCPf_S3kADBf"
      },
      "outputs": [],
      "source": [
        "from retinaface import RetinaFace\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "def detect_and_crop_face(image_path, target_size=(224, 224)):\n",
        "    \"\"\"\n",
        "    Detect and crop faces from an image using RetinaFace.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        detections = RetinaFace.detect_faces(image_path)\n",
        "        if not isinstance(detections, dict) or len(detections) == 0:\n",
        "            return None\n",
        "\n",
        "        # Extract face coordinates\n",
        "        face_key = list(detections.keys())[0]\n",
        "        x1, y1, x2, y2 = detections[face_key]['facial_area']\n",
        "        img = cv2.imread(image_path)\n",
        "        if img is None or img.size == 0:\n",
        "            return None\n",
        "\n",
        "        cropped_face = img[y1:y2, x1:x2]\n",
        "        cropped_face = cv2.resize(cropped_face, target_size)\n",
        "        return cropped_face\n",
        "    except Exception as e:\n",
        "        return None\n",
        "\n",
        "# Testing the method\n",
        "image_path = \"/content/DnHFaces/open_data_set/photos_all_faces/a_gp_0_ef_01.jpg\"\n",
        "cropped_face = detect_and_crop_face(image_path)\n",
        "if cropped_face is not None:\n",
        "    print(\"Cropped face detected successfully.\")\n",
        "    cv2_imshow(cropped_face)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VR__HLIWK2SG"
      },
      "outputs": [],
      "source": [
        "def preprocess_with_retinaface(folder_path, valid_image_types, resize_to=(224, 224)):\n",
        "    images, labels, metadata_list = [], [], []\n",
        "    for root, _, files in os.walk(folder_path):\n",
        "        for file in files:\n",
        "            if file.endswith(\".jpg\"):\n",
        "                metadata = parse_filename(file)\n",
        "                if metadata[\"image_type\"] in valid_image_types:\n",
        "                    img_path = os.path.join(root, file)\n",
        "                    cropped_face = detect_and_crop_face(img_path, target_size=resize_to)\n",
        "                    if cropped_face is not None:\n",
        "                        images.append(cropped_face)\n",
        "                        labels.append(metadata[\"subject_id\"])\n",
        "                        metadata_list.append(metadata)\n",
        "    return images, labels, metadata_list\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UONXiqyN_j8Q"
      },
      "source": [
        "# Generate Proxy Portraits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBL7mdFtPBpX"
      },
      "outputs": [],
      "source": [
        "# def generate_proxy_portraits(images, labels, metadata, missing_subjects, valid_image_types=[\"ef\", \"porF\"]):\n",
        "#     \"\"\"\n",
        "#     Generate proxy portraits for missing subjects using images from other folders.\n",
        "#     \"\"\"\n",
        "#     proxy_images = []\n",
        "#     proxy_labels = []\n",
        "\n",
        "#     for img, label, meta in zip(images, labels, metadata.to_dict(orient=\"records\")):\n",
        "#         if label in missing_subjects and meta[\"image_type\"] in valid_image_types:\n",
        "#             proxy_images.append(img)\n",
        "#             proxy_labels.append(label)\n",
        "\n",
        "#     return np.array(proxy_images), np.array(proxy_labels)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suOAddN8AGXd"
      },
      "source": [
        "# Preprocess Dataset and Handle Missing Portraits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWppWxDXAKWp"
      },
      "outputs": [],
      "source": [
        "def generate_proxy_portraits(images, labels, metadata, missing_subjects, valid_image_types=[\"ef\", \"porF\"]):\n",
        "    proxy_images, proxy_labels = [], []\n",
        "    for img, label, meta in zip(images, labels, metadata):\n",
        "        if label in missing_subjects and meta[\"image_type\"] in valid_image_types:\n",
        "            proxy_images.append(img)\n",
        "            proxy_labels.append(label)\n",
        "    return proxy_images, proxy_labels\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5xqbi0nAMA9"
      },
      "source": [
        "# Preprocessing Valid Image Types"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# # Step 1: Preprocess dataset (already implemented)\n",
        "# data_folder = \"/content/DnHFaces/open_data_set\"\n",
        "# valid_image_types = [\"ef\", \"por\", \"porF\", \"porL\", \"porR\"]\n",
        "\n",
        "# images, labels, metadata = preprocess_with_retinaface(data_folder, valid_image_types)\n",
        "\n",
        "# # Detect missing subjects in portraits and handle them\n",
        "# portraits_subjects = metadata[metadata[\"image_type\"] == \"por\"][\"subject_id\"].unique()\n",
        "# missing_subjects = set(metadata[\"subject_id\"].unique()) - set(portraits_subjects)\n",
        "\n",
        "# proxy_images, proxy_labels = generate_proxy_portraits(images, labels, metadata, missing_subjects)\n",
        "\n",
        "# # Combine original and proxy portraits\n",
        "# original_images = images[metadata[\"image_type\"] == \"por\"]\n",
        "# original_labels = labels[metadata[\"image_type\"] == \"por\"]\n",
        "\n",
        "# combined_images = np.concatenate([original_images, proxy_images], axis=0)\n",
        "# combined_labels = np.concatenate([original_labels, proxy_labels], axis=0)\n",
        "\n"
      ],
      "metadata": {
        "id": "q8ZX-lgj3mQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_KnkO4KDbYH"
      },
      "source": [
        "# Label Mapping"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Create a mapping from subject IDs to numerical labels\n",
        "# unique_labels = sorted(set(combined_labels))  # Get all unique labels\n",
        "# label_to_int = {label: i for i, label in enumerate(unique_labels)}  # Map each label to an integer\n",
        "\n",
        "# # Apply the mapping to convert labels to integers\n",
        "# encoded_labels = np.array([label_to_int[label] for label in combined_labels])"
      ],
      "metadata": {
        "id": "KWjHmFJOryrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDaVopSRKI4w"
      },
      "source": [
        "# Data Partitioning (5-fold Cross-Validation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JeizfdFhBB1Q"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import KFold, train_test_split\n",
        "import tensorflow as tf\n",
        "\n",
        "def k_fold_partition(images, labels, metadata, k=5):\n",
        "    unique_subject_ids = list(set(labels))\n",
        "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "\n",
        "    folds = []\n",
        "    for train_val_idx, test_idx in kf.split(unique_subject_ids):\n",
        "        train_val_ids = [unique_subject_ids[i] for i in train_val_idx]\n",
        "        test_ids = [unique_subject_ids[i] for i in test_idx]\n",
        "        train_ids, val_ids = train_test_split(train_val_ids, test_size=0.2, random_state=42)\n",
        "\n",
        "        # Partition data\n",
        "        train_data, train_labels = [], []\n",
        "        val_data, val_labels = [], []\n",
        "        test_data, test_labels = [], []\n",
        "\n",
        "        for img, label in zip(images, labels):\n",
        "            if label in train_ids:\n",
        "                train_data.append(img)\n",
        "                train_labels.append(label)\n",
        "            elif label in val_ids:\n",
        "                val_data.append(img)\n",
        "                val_labels.append(label)\n",
        "            elif label in test_ids:\n",
        "                test_data.append(img)\n",
        "                test_labels.append(label)\n",
        "\n",
        "        folds.append({\n",
        "            \"train\": (train_data, train_labels),\n",
        "            \"val\": (val_data, val_labels),\n",
        "            \"test\": (test_data, test_labels)\n",
        "        })\n",
        "    return folds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPfgfAooRSmx"
      },
      "outputs": [],
      "source": [
        "# # Validate all images before training\n",
        "# for fold in folds:\n",
        "#     train_data, train_labels = fold[\"train\"]\n",
        "#     for img in train_data:\n",
        "#         assert img.shape == (224, 224, 3), f\"Image shape mismatch: {img.shape}\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xioqXZKpGTIx"
      },
      "source": [
        "# Adaface Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AlUXo6DQGVcl"
      },
      "outputs": [],
      "source": [
        "class AdaFaceLoss(tf.keras.losses.Loss):\n",
        "    def __init__(self, scale=64, margin=0.5, name=\"AdaFaceLoss\"):\n",
        "        super(AdaFaceLoss, self).__init__(name=name)\n",
        "        self.scale = scale\n",
        "        self.margin = margin\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        norm_logits = tf.norm(y_pred, axis=1, keepdims=True)\n",
        "        quality_indicator = tf.sigmoid(norm_logits)\n",
        "        adaptive_margin = self.margin * (1 - quality_indicator)\n",
        "        logits_with_margin = self.scale * (tf.cos(y_pred - adaptive_margin) - adaptive_margin)\n",
        "        cross_entropy_loss = tf.nn.softmax_cross_entropy_with_logits(labels=y_true, logits=logits_with_margin)\n",
        "        return tf.reduce_mean(cross_entropy_loss)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# import torch\n",
        "# from torchvision import transforms\n",
        "\n",
        "# # Define a preprocessing pipeline for the input images\n",
        "# def preprocess_images(images):\n",
        "#     \"\"\"\n",
        "#     Preprocess images for ViT input: resize, normalize, and permute dimensions.\n",
        "#     \"\"\"\n",
        "#     # Convert NumPy arrays to PyTorch tensors\n",
        "#     images = np.array(images, dtype=np.float32)  # Ensure float32 for PyTorch compatibility\n",
        "#     images = torch.tensor(images).permute(0, 3, 1, 2)  # Change shape to (batch_size, channels, height, width)\n",
        "\n",
        "#     # Normalize images to match ImageNet pretrained weights\n",
        "#     transform = transforms.Normalize(\n",
        "#         mean=[0.485, 0.456, 0.406],  # ImageNet means\n",
        "#         std=[0.229, 0.224, 0.225]    # ImageNet stds\n",
        "#     )\n",
        "#     images = transform(images / 255.0)  # Normalize to [0, 1] and apply ImageNet normalization\n",
        "#     return images\n"
      ],
      "metadata": {
        "id": "nkrYllgRLR9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n45BMkPRLQPG"
      },
      "source": [
        "# ViT Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqAUaA3NLSbG"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers, Model\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "\n",
        "def build_vit_model(input_shape=(224, 224, 3), num_classes=11):\n",
        "    base_model = EfficientNetB0(weights=\"imagenet\", include_top=False, input_shape=input_shape, pooling=\"avg\")\n",
        "    inputs = tf.keras.Input(shape=input_shape)\n",
        "    x = layers.Dropout(0.5)(base_model(inputs, training=False))\n",
        "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "    return Model(inputs, outputs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3sw7qpvLUOM"
      },
      "source": [
        "# Ranking Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gfgq8SJQLWbX"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# def compute_rank1_accuracy(probe_embeddings, probe_labels, gallery_embeddings, gallery_labels):\n",
        "#     \"\"\"\n",
        "#     Compute the Rank-1 accuracy by comparing probe embeddings to gallery embeddings.\n",
        "#     \"\"\"\n",
        "#     similarities = cosine_similarity(probe_embeddings, gallery_embeddings)\n",
        "#     correct_matches = 0\n",
        "\n",
        "#     for i, probe_label in enumerate(probe_labels):\n",
        "#         # Find the index of the most similar gallery image\n",
        "#         most_similar_idx = np.argmax(similarities[i])\n",
        "#         predicted_label = gallery_labels[most_similar_idx]\n",
        "\n",
        "#         # Check if the predicted label matches the probe label\n",
        "#         if predicted_label == probe_label:\n",
        "#             correct_matches += 1\n",
        "\n",
        "#     rank1_accuracy = correct_matches / len(probe_labels)\n",
        "#     return rank1_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zcx58-WhLcF9"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Assuming `train_labels` and `val_labels` contain string labels (e.g., ['h', 'k', 'l'])\n",
        "all_labels = np.concatenate([train_labels, val_labels])\n",
        "unique_labels = np.unique(all_labels)  # Get unique string labels\n",
        "\n",
        "# Create a mapping from label strings to integers\n",
        "label_to_int = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "\n",
        "# Map string labels to integers\n",
        "train_labels_int = np.array([label_to_int[label] for label in train_labels])\n",
        "val_labels_int = np.array([label_to_int[label] for label in val_labels])\n",
        "\n",
        "# Convert integer labels to one-hot encoding\n",
        "train_labels_one_hot = tf.keras.utils.to_categorical(train_labels_int, num_classes=len(unique_labels))\n",
        "val_labels_one_hot = tf.keras.utils.to_categorical(val_labels_int, num_classes=len(unique_labels))\n",
        "\n",
        "print(\"One-hot encoding successful!\")\n"
      ],
      "metadata": {
        "id": "QMGXvtdoXrxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFOpWjicLevh"
      },
      "outputs": [],
      "source": [
        "for fold in folds:\n",
        "    # Unpack train and validation data\n",
        "    train_data, train_labels = fold[\"train\"]\n",
        "    val_data, val_labels = fold[\"val\"]\n",
        "\n",
        "    # Calculate unique labels\n",
        "    all_labels = np.concatenate([train_labels, val_labels])\n",
        "    unique_labels = np.unique(all_labels)\n",
        "\n",
        "    # Convert labels to one-hot encoding\n",
        "    train_labels = tf.keras.utils.to_categorical(train_labels, num_classes=len(unique_labels))\n",
        "    val_labels = tf.keras.utils.to_categorical(val_labels, num_classes=len(unique_labels))\n",
        "\n",
        "    print(\"Training Fold 1...\")\n",
        "    # Proceed with model training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@"
      ],
      "metadata": {
        "id": "qoe1eu-rXYzV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "lepbuwrsTwmL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, test_data, test_labels, gallery_data, gallery_labels):\n",
        "    \"\"\"\n",
        "    Evaluate the model using rank-1 accuracy.\n",
        "    \"\"\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    test_data, test_labels = test_data.to(device), test_labels.to(device)\n",
        "    gallery_data, gallery_labels = gallery_data.to(device), gallery_labels.to(device)\n",
        "\n",
        "    # Get embeddings for test and gallery data\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        _, test_embeddings = model(test_data)\n",
        "        _, gallery_embeddings = model(gallery_data)\n",
        "\n",
        "    # Calculate rank-1 accuracy\n",
        "    rank_1_accuracy = evaluate_ranking(test_embeddings, test_labels, gallery_embeddings, gallery_labels)\n",
        "    print(f\"Rank-1 Accuracy: {rank_1_accuracy * 100:.2f}%\")\n",
        "    return rank_1_accuracy\n"
      ],
      "metadata": {
        "id": "AS4yogN_TwBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main_training_evaluation(folds, num_classes):\n",
        "    \"\"\"\n",
        "    Perform training and evaluation for each fold using cross-validation.\n",
        "    \"\"\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Create a mapping from unique labels to integer indices\n",
        "    unique_labels = sorted(set(label for fold in folds for label in fold[\"train\"][1]))\n",
        "    label_to_idx = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "\n",
        "    # Iterate over each fold\n",
        "    for fold_idx, fold in enumerate(folds):\n",
        "        print(f\"\\n=== Training Fold {fold_idx + 1} ===\")\n",
        "\n",
        "        # Load fold data\n",
        "        train_data, train_labels = fold[\"train\"]\n",
        "        val_data, val_labels = fold[\"val\"]\n",
        "        test_data, test_labels = fold[\"test\"]\n",
        "\n",
        "        # Map string labels to integer indices\n",
        "        train_labels = [label_to_idx[label] for label in train_labels]\n",
        "        val_labels = [label_to_idx[label] for label in val_labels]\n",
        "        test_labels = [label_to_idx[label] for label in test_labels]\n",
        "\n",
        "        # Convert data and labels to PyTorch tensors\n",
        "        train_data, train_labels = torch.tensor(train_data).float(), torch.tensor(train_labels).long()\n",
        "        val_data, val_labels = torch.tensor(val_data).float(), torch.tensor(val_labels).long()\n",
        "        test_data, test_labels = torch.tensor(test_data).float(), torch.tensor(test_labels).long()\n",
        "\n",
        "        # Initialize model, loss function, and optimizer\n",
        "        model = ViTModel(num_classes=num_classes).to(device)\n",
        "        loss_function = AdaFaceLoss(embedding_size=512, num_classes=num_classes)\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
        "\n",
        "        # Train the model\n",
        "        model = train_model(\n",
        "            model, train_data, train_labels, val_data, val_labels,\n",
        "            loss_function, optimizer, num_epochs=10\n",
        "        )\n",
        "\n",
        "        # Evaluate the model\n",
        "        print(f\"\\n=== Evaluating Fold {fold_idx + 1} ===\")\n",
        "        gallery_data, gallery_labels = val_data, val_labels  # Use validation set as gallery for simplicity\n",
        "        rank_1_accuracy = evaluate_model(model, test_data, test_labels, gallery_data, gallery_labels)\n"
      ],
      "metadata": {
        "id": "jYxmAETMT06U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main Pipeline"
      ],
      "metadata": {
        "id": "hQbP45p2Dbvo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    folder_path = \"/content/DnHFaces/open_data_set\"\n",
        "    valid_image_types = [\"ef\", \"por\", \"porF\", \"porL\", \"porR\"]\n",
        "\n",
        "    # Preprocess the dataset\n",
        "    images, labels, metadata = preprocess_with_retinaface(folder_path, valid_image_types)\n",
        "\n",
        "    # Generate 5-fold cross-validation splits\n",
        "    folds = k_fold_partition(images, labels, metadata)\n",
        "\n",
        "    # Determine the number of unique classes\n",
        "    num_classes = len(set(labels))\n",
        "\n",
        "    # Train and evaluate the model\n",
        "    main_training_evaluation(folds, num_classes)\n"
      ],
      "metadata": {
        "id": "MJX7ELVmDe0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ============================================================================================================================="
      ],
      "metadata": {
        "id": "gc5Xcl4htHrK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDXrcjVSBEsQ"
      },
      "source": [
        "# Training Phase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zPy_R6RaBMVW"
      },
      "outputs": [],
      "source": [
        "# Step 3: Define the Vision Transformer (ViT) model with AdaFace loss\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "\n",
        "def build_vit_model(input_shape=(224, 224, 3), num_classes=11):\n",
        "    \"\"\"\n",
        "    Build a Vision Transformer (ViT) model with an AdaFace loss.\n",
        "    \"\"\"\n",
        "    # Base model (EfficientNet or ViT backbone)\n",
        "    base_model = EfficientNetB0(include_top=False, input_shape=input_shape, pooling=\"avg\")\n",
        "\n",
        "    # MLP Head\n",
        "    inputs = tf.keras.Input(shape=input_shape)\n",
        "    features = base_model(inputs, training=False)\n",
        "    outputs = layers.Dense(num_classes, activation=\"softmax\")(features)\n",
        "\n",
        "    return Model(inputs, outputs)\n",
        "\n",
        "# Instantiate the model\n",
        "vit_model = build_vit_model()\n",
        "\n",
        "# Compile the model with AdaFace Loss\n",
        "vit_model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "    loss=AdaFaceLoss(scale=64, margin=0.5, regularizer_weight=0.01),  # AdaFace loss\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gj59WODNC2-7"
      },
      "source": [
        "# Validation Phase\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BektO_XcC2eL"
      },
      "outputs": [],
      "source": [
        "# Step 4: Train and validate the model for each fold\n",
        "rank_1_accuracies = []\n",
        "\n",
        "for i, fold in enumerate(folds):\n",
        "    print(f\"Training Fold {i + 1}...\")\n",
        "\n",
        "    train_data, train_labels = fold[\"train\"]\n",
        "    val_data, val_labels = fold[\"val\"]\n",
        "\n",
        "    vit_model.fit(\n",
        "        train_data,\n",
        "        tf.keras.utils.to_categorical(train_labels, num_classes=len(unique_labels)),\n",
        "        validation_data=(val_data, tf.keras.utils.to_categorical(val_labels, num_classes=len(unique_labels))),\n",
        "        epochs=10,\n",
        "        batch_size=32\n",
        "    )\n",
        "\n",
        "\n",
        "    # Validation metrics\n",
        "    val_predictions = vit_model.predict(val_data)\n",
        "    val_accuracy = tf.keras.metrics.CategoricalAccuracy()\n",
        "    val_accuracy.update_state(tf.keras.utils.to_categorical(val_labels), val_predictions)\n",
        "    print(f\"Validation Accuracy for Fold {i + 1}: {val_accuracy.result().numpy()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "si6V0tAmGbKp"
      },
      "source": [
        "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzffClqkKNyb"
      },
      "source": [
        "# Training Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "olpZvYZMKQ9p"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# Training function\n",
        "def train(model, dataloader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in dataloader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    return running_loss / len(dataloader)\n",
        "\n",
        "# Validation function\n",
        "def validate(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in dataloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    return running_loss / len(dataloader), accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQsEgU65KWU6"
      },
      "source": [
        "# Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvkJ_UFFKV8z"
      },
      "outputs": [],
      "source": [
        "# Set number of epochs\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train(model, dataloader, criterion, optimizer, device)\n",
        "    val_loss, val_accuracy = validate(model, dataloader, criterion, device)\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%')\n",
        "\n",
        "    # Early stopping condition\n",
        "    if val_accuracy >= 100.0:\n",
        "        print(f\"Reached 100% accuracy at epoch {epoch+1}. Stopping early.\")\n",
        "        break\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Sybd3FnsblA"
      },
      "source": [
        "# Data Preperation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KyUcgDLYsfE7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "class DroneFaceDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.image_paths = []\n",
        "        self.labels = []\n",
        "        self.subject_label_map = {}\n",
        "        current_label = 0\n",
        "\n",
        "        # Iterate through each subfolder (e.g., 'photos_all', 'photos_all_faces', etc.)\n",
        "        for subfolder in os.listdir(root_dir):\n",
        "            subfolder_path = os.path.join(root_dir, subfolder)\n",
        "            if os.path.isdir(subfolder_path):\n",
        "                for filename in os.listdir(subfolder_path):\n",
        "                    if filename.endswith(('.jpg', '.png', '.jpeg')):\n",
        "                        # Handle single-subject folders (e.g., 'photos_all_faces', 'portraits', etc.)\n",
        "                        if subfolder != 'photos_all':\n",
        "                            subject = filename[0].lower()  # Extract the first letter as the subject label\n",
        "                            if subject not in self.subject_label_map:\n",
        "                                self.subject_label_map[subject] = current_label\n",
        "                                current_label += 1\n",
        "                            label = self.subject_label_map[subject]\n",
        "                            self.image_paths.append(os.path.join(subfolder_path, filename))\n",
        "                            self.labels.append(label)\n",
        "                        # Handle the 'photos_all' folder with combined subjects (e.g., 'ab')\n",
        "                        else:\n",
        "                            subjects_combination = filename[:2].lower()  # Extract the first two letters\n",
        "                            # For simplicity, we treat combined subjects as multi-class with a placeholder label\n",
        "                            label = self.subject_label_map.get(subjects_combination[0], -1)\n",
        "                            self.image_paths.append(os.path.join(subfolder_path, filename))\n",
        "                            self.labels.append(label)  # Adjust this for more complex handling if needed\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        label = self.labels[idx]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "# Define transformations for the images\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize for ViT input\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "# Load the dataset\n",
        "dataset = DroneFaceDataset(root_dir='/content/DnHFaces/open_data_set', transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQxbYbyxPGc0"
      },
      "source": [
        "# Load ViT Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxkE46D2pLee"
      },
      "outputs": [],
      "source": [
        "import timm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Load a pre-trained ViT model\n",
        "model = timm.create_model('vit_base_patch16_224', pretrained=True)\n",
        "\n",
        "# Update the classifier head for the number of classes (subjects) in the dataset\n",
        "num_classes = len(set(dataset.labels))  # Number of unique subjects\n",
        "model.head = nn.Linear(model.head.in_features, num_classes)\n",
        "\n",
        "# Move the model to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7f8mQ6Zjpen1"
      },
      "source": [
        "# AdaFace Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXEvSmobPF6d"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class AdaFaceLoss(nn.Module):\n",
        "    def __init__(self, s=64.0, m=0.4):\n",
        "        super(AdaFaceLoss, self).__init__()\n",
        "        self.s = s  # Scaling factor\n",
        "        self.m = m  # Margin\n",
        "\n",
        "    def forward(self, embeddings, labels):\n",
        "        # Normalize the embeddings\n",
        "        embeddings = F.normalize(embeddings)\n",
        "\n",
        "        # Compute cosine similarity between embeddings\n",
        "        cosine_similarity = F.linear(embeddings, embeddings)\n",
        "\n",
        "        # Adjust the margin for positive pairs (same class)\n",
        "        positive_margin = cosine_similarity - self.m\n",
        "        logits = torch.where(labels.unsqueeze(1) == labels.unsqueeze(0), positive_margin, cosine_similarity)\n",
        "\n",
        "        # Scale the logits\n",
        "        logits = self.s * logits\n",
        "\n",
        "        # Apply cross-entropy loss on the scaled logits\n",
        "        return F.cross_entropy(logits, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zM-Ma6KPmrl"
      },
      "source": [
        "# Training + Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPHwYpQoPqE1"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = AdaFaceLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# Training function\n",
        "def train(model, dataloader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in dataloader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        embeddings = model(images)\n",
        "        loss = criterion(embeddings, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    return running_loss / len(dataloader)\n",
        "\n",
        "# Validation function\n",
        "def validate(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in dataloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            embeddings = model(images)\n",
        "            loss = criterion(embeddings, labels)\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            _, predicted = torch.max(embeddings, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    return running_loss / len(dataloader), accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NZCGG3NPtUU"
      },
      "source": [
        "# Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0eKxAbeEP2Ma"
      },
      "outputs": [],
      "source": [
        "# Set number of epochs\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train(model, dataloader, criterion, optimizer, device)\n",
        "    val_loss, val_accuracy = validate(model, dataloader, criterion, device)\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%')\n",
        "\n",
        "    # Early stopping condition\n",
        "    if val_accuracy >= 100.0:\n",
        "        print(f\"Reached 100% accuracy at epoch {epoch+1}. Stopping early.\")\n",
        "        break\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}